import requests
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
from collections import OrderedDict
import streamlit as st
import pandas as pd
import time

def timer(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f"{func.__name__} took {end_time - start_time:.2f} seconds")
        return result
    return wrapper

@timer
def get_subpages(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"Error fetching subpages for {url}: {e}")
        return []

    soup = BeautifulSoup(response.text, 'html.parser')
    links = soup.find_all('a')
    subpages = [urljoin(url, link.get('href').split('#')[0]) for link in links if link.get('href') and '#' not in link.get('href')]
    return list(set(subpages))  # é‡è¤‡ã‚’é™¤å»

@timer
def search_goo_gl_urls(url, domain):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"Error searching goo.gl URLs for {url}: {e}")
        return [], f"ã‚¨ãƒ©ãƒ¼: {str(e)}"

    soup = BeautifulSoup(response.text, 'html.parser')
    goo_gl_pattern = re.compile(r'https?://goo\.gl/\S+')
    goo_gl_urls = goo_gl_pattern.findall(str(soup))

    parsed_url = urlparse(url)
    if parsed_url.netloc != domain:
        return [], None

    print(f"Found {len(goo_gl_urls)} goo.gl URLs on {url}")
    return list(set(goo_gl_urls)), ''

@timer
def process_subpages(subpages, domain, progress_bar, progress_text):
    goo_gl_urls = []
    site_urls = set()
    total_subpages = len(subpages)
    for i, subpage in enumerate(subpages):
        try:
            subpage_goo_gl_urls, _ = search_goo_gl_urls(subpage, domain)
            goo_gl_urls.extend(subpage_goo_gl_urls)
            if subpage_goo_gl_urls:
                site_urls.add(subpage)
        except Exception as exc:
            print(f'{subpage} generated an exception: {exc}')
        
        # Update progress
        progress = (i + 1) / total_subpages
        progress_bar.progress(progress)
        progress_text.text(f"ã‚µãƒ–ãƒšãƒ¼ã‚¸å‡¦ç†ä¸­: {i+1}/{total_subpages}")
    return goo_gl_urls, site_urls

@timer
def process_url(url, progress_bar, progress_text):
    url = url.strip()
    parsed_url = urlparse(url)
    domain = parsed_url.netloc

    if not domain:
        return None

    result = {"URL": url, "goo.gl URLs": [], "ã‚¨ãƒ©ãƒ¼": '', "ã‚µã‚¤ãƒˆURL": set()}

    progress_text.text(f"ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸å‡¦ç†ä¸­: {url}")
    main_goo_gl_urls, error = search_goo_gl_urls(url, domain)
    if error:
        result["ã‚¨ãƒ©ãƒ¼"] = error
    else:
        result["goo.gl URLs"].extend(main_goo_gl_urls)

    progress_text.text(f"ã‚µãƒ–ãƒšãƒ¼ã‚¸å–å¾—ä¸­: {url}")
    subpages = get_subpages(url)
    
    progress_text.text(f"ã‚µãƒ–ãƒšãƒ¼ã‚¸å‡¦ç†ä¸­: {url}")
    subpage_goo_gl_urls, subpage_site_urls = process_subpages(subpages, domain, progress_bar, progress_text)
    result["goo.gl URLs"].extend(subpage_goo_gl_urls)
    result["ã‚µã‚¤ãƒˆURL"].update(subpage_site_urls)

    return domain, result

@timer
def process_urls(urls):
    results = OrderedDict()
    progress_bar = st.progress(0)
    progress_text = st.empty()
    
    total_urls = len(urls)
    for i, url in enumerate(urls):
        try:
            progress_text.text(f"URLå‡¦ç†ä¸­ ({i+1}/{total_urls}): {url}")
            result = process_url(url, progress_bar, progress_text)
            if result:
                domain, data = result
                results[domain] = data
        except Exception as exc:
            print(f'{url} generated an exception: {exc}')
            results[url] = {"URL": url, "goo.gl URLs": [], "ã‚¨ãƒ©ãƒ¼": str(exc), "ã‚µã‚¤ãƒˆURL": set()}
        
        # Update overall progress
        progress_bar.progress((i + 1) / total_urls)
    
    progress_text.text("å‡¦ç†å®Œäº†")
    return results

# Streamlit UI
st.set_page_config(page_title='å…¨ãƒšãƒ¼ã‚¸æ¤œç´¢ goo.gl æ¤œå“ãƒ„ãƒ¼ãƒ«', page_icon='ğŸ”')
st.title('å…¨ãƒšãƒ¼ã‚¸æ¤œç´¢ goo.gl æ¤œå“ãƒ„ãƒ¼ãƒ«')

st.markdown("""
<style>
.small-font {
    font-size:0.8em;
}
.big-font {
    font-size:1em;
}
.mb40{
    margin-bottom: 40px;
}
</style>
""", unsafe_allow_html=True)

st.markdown("""
<p class="big-font">
ç„¡æ–™ã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚ãƒ¡ãƒ¢ãƒªå®¹é‡ã‚ªãƒ¼ãƒãƒ¼ã§å¼·åˆ¶çµ‚äº†ãŒé »ç™ºã—ã¦ã„ã¾ã™ã€‚<br>æ¤œç´¢ä»¶æ•°1ï½3ä»¶ã§ã”ä½¿ç”¨ãã ã•ã„ã¾ã›ã€‚</p>
<div class="small-font mb40">
â€» ç›®è¦–ã§é€²æ—ãŒåˆ†ã‹ã‚‹ã‚ˆã†ã«èª¿æ•´ã—ã¾ã—ãŸã€‚<br>
â€» å°‘ã—ã ã‘è»½ãã™ã‚‹ã‚ˆã†ã«èª¿æ•´ã—ã¾ã—ãŸãŒã€åŒæ™‚æ¥ç¶šçŠ¶æ³ã«ã‚ˆã£ã¦ã¯å†åº¦å¼·åˆ¶çµ‚äº†ã«ãªã‚‹ã‹ã‚‚ã—ã‚Œãªã„ãŸã‚ã€<br>ã€€å¼•ãç¶šãæ¤œç´¢ä»¶æ•°1ï½3ä»¶ã§ãŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚
</div>
""", unsafe_allow_html=True)

urls = st.text_area('èª¿æŸ»ã™ã‚‹ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®URLã‚’1è¡Œã«1ã¤ãšã¤å…¥åŠ›ã—ã¦ãã ã•ã„:')

if st.button('æ¤œç´¢'):
    if urls:
        urls_list = [url.strip() for url in urls.split('\n') if url.strip()]
        with st.spinner('æ¤œç´¢ä¸­...'):
            results = process_urls(urls_list)
        
        # çµæœã®è¡¨ç¤º
        if results:
            rows = []
            for domain, data in results.items():
                goo_gl_urls = '\n'.join(set(data['goo.gl URLs']))
                site_urls = '\n'.join(data['ã‚µã‚¤ãƒˆURL'])
                row = {'ãƒ‰ãƒ¡ã‚¤ãƒ³': domain, 'URL': data['URL'], 'goo.gl URLs': goo_gl_urls, 'ã‚µã‚¤ãƒˆURL': site_urls, 'ã‚¨ãƒ©ãƒ¼': data['ã‚¨ãƒ©ãƒ¼']}
                rows.append(row)
            df = pd.DataFrame(rows)
            st.dataframe(df.style.set_properties(**{'text-align': 'left'}))
        
            csv = df.to_csv().encode('utf-8')
            st.download_button(
                label="çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name="goo_gl_search_results.csv",
                mime="text/csv",
            )
        else:
            st.warning("goo.glãƒªãƒ³ã‚¯ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        st.warning('URLã‚’å°‘ãªãã¨ã‚‚1ã¤å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚')